{"cells":[{"cell_type":"markdown","metadata":{"id":"BAFIVBg9nl4t"},"source":["# Import Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NuCGj-Ck7ov3"},"outputs":[],"source":["from PIL import Image, ImageDraw, ImageFont\n","# from bs4 import BeautifulSoup\n","import math\n","import random\n","import string\n","import requests\n","import numpy as np\n","import math\n","import os\n","import cv2\n","import imageio\n","import shutil"]},{"cell_type":"markdown","metadata":{"id":"fVbc5OPrnpX2"},"source":["# Import Word Lists for Text Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ctYLgKXbtgQ"},"outputs":[],"source":["# import English word list (10000 entries) for random text generation\n","with open('/content/drive/MyDrive/Gradient Health/Text Detection Project/Word Lists/English Word List.txt') as f:\n","    English_list = f.read().split('\\n')[:-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbLmTYPEi04s"},"outputs":[],"source":["# # webscrape Portuguese word list\n","# page = requests.get('https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/Portuguese_wordlist')\n","# soup = BeautifulSoup(page.text)\n","# words = soup.find_all('li')\n","# Portuguese_list = [word.get_text().split(' ')[0]for word in words[:4927]]\n","\n","# # save to local\n","# with open('/content/drive/MyDrive/Gradient Health/Text Detection Project/Word Lists/Portuguese Word List.txt', 'w') as f:\n","#     for word in Portuguese_list:\n","#         f.write(f'{word}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M42ytAX8spyK"},"outputs":[],"source":["# import Portuguese word list (4927 entries) for random text generation\n","with open('/content/drive/MyDrive/Gradient Health/Text Detection Project/Word Lists/Portuguese Word List.txt') as f:\n","    Portuguese_list = f.read().split('\\n')[:-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSdCj3nOz2bY"},"outputs":[],"source":["# # clean Chinese word list\n","# with open('/content/drive/MyDrive/Gradient Health/Text Detection Project/Word Lists/Chinese Word List Uncleaned.txt') as source:\n","#     with open('/content/drive/MyDrive/Gradient Health/Text Detection Project/Word Lists/Chinese Word List.txt', 'w') as dest:\n","#         for line in source:\n","#             words = line.split()\n","#             if words:\n","#                 dest.write(words[0] + '\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DwbFadTZ2MmU"},"outputs":[],"source":["# import Chinese word list (56064 entries) for random text generation\n","with open('/content/drive/MyDrive/Gradient Health/Text Detection Project/Word Lists/Chinese Word List.txt') as f:\n","    Chinese_list = f.read().split('\\n')[:-1]"]},{"cell_type":"markdown","metadata":{"id":"cyYZbOlNnwa2"},"source":["# Helper Functions for Random Text Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XIEzVAnV87rN"},"outputs":[],"source":["# generate random text block\n","# supports English, Portuguese, and Chinese\n","def generate_random_text(length, language):\n","    if language == 'English':\n","        word_list = English_list\n","        text = ' '.join(random.choice(word_list) for _ in range(length))\n","        return text\n","    elif language == 'Portuguese':\n","        word_list = Portuguese_list\n","        text = ' '.join(random.choice(word_list) for _ in range(length))\n","        return text\n","    elif language == 'Chinese':\n","        word_list = Chinese_list\n","        text = ' '.join(random.choice(word_list) for _ in range(length))\n","        return text\n","    else:\n","        return 'Language not supported'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vz90QLPK9boK"},"outputs":[],"source":["# generate a random color in RGB format\n","def generate_random_color():\n","    r = random.randint(0, 255)\n","    g = random.randint(0, 255)\n","    b = random.randint(0, 255)\n","    return (r, g, b)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9CKvVH3sE75C"},"outputs":[],"source":["# generate a random grayscale color in RGB format\n","def generate_random_grayscale_color():\n","    intensity = random.randint(0, 255)\n","    return (intensity, intensity, intensity)"]},{"cell_type":"code","source":["# generate a number from Poisson distribution\n","def generate_poisson(lower_bound = 1, upper_bound = 10, lambd = 2):\n","    while True:\n","        sample = np.random.poisson(lambd)\n","        if lower_bound <= sample <= upper_bound:\n","            return sample"],"metadata":{"id":"5bTQuvYsjiXC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OhHa04FRn70a"},"source":["# Main Function for Random Text Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQG-BhSv9fR6"},"outputs":[],"source":["# overlay random text on image\n","# customize randomness\n","def overlay_random_text(\n","    input, output,\n","    language = 'English',\n","    font_name =  'Arial Unicode.ttf',\n","    font_size = 20, # pixels\n","    text_length = 10, # number of characters\n","    fill_color = (0, 0, 0), # default black\n","    stroke_color = (255, 255, 255), # default white\n","    stroke_width = 1, # pixels\n","    rotation_degree = 0):\n","\n","    with Image.open(input) as img:\n","\n","        # makes sure image is in RGB mode\n","        if img.mode != 'RGB':\n","            img = img.convert('RGB')\n","\n","        draw = ImageDraw.Draw(img)\n","        font = ImageFont.truetype('/content/drive/MyDrive/Gradient Health/Text Detection Project/Fonts/' + font_name, font_size)\n","        text = generate_random_text(text_length, language)\n","\n","        # generate random coordinate (top-left)\n","        x, y = random.randint(0, img.width), random.randint(0, img.height)\n","\n","        # generate transparent image to temporarily place text\n","        temp = Image.new('RGBA', (img.width, img.height), (0, 0, 0, 0))  # transparent black background\n","\n","        # draw text\n","        temp_draw = ImageDraw.Draw(temp)\n","        temp_draw.text((x, y), text, font=font, fill=fill_color, stroke_width=stroke_width, stroke_fill=stroke_color)\n","\n","        # rotate\n","        temp = temp.rotate(rotation_degree, fillcolor=(0, 0, 0, 0))\n","\n","        # find the first and last pixels containing text on y axis and x axis\n","        data = np.array(temp)\n","        colored = np.any(data[:, :, :] != [0, 0, 0, 0], axis=2)  # find pixels that are not transparent black\n","        ## y axis\n","        reduced_y = np.any(colored, axis=1)\n","        indices_y = np.where(reduced_y)[0]\n","        first_index_y = indices_y[0] if indices_y.size else 0\n","        last_index_y = indices_y[-1] if indices_y.size else 0\n","        ## x axis\n","        reduced_x = np.any(colored, axis=0)\n","        indices_x = np.where(reduced_x)[0]\n","        first_index_x = indices_x[0] if indices_x.size else 0\n","        last_index_x = indices_x[-1] if indices_x.size else 0\n","\n","        # # uncomment when checking manual bounding boxes\n","        # # draw bounding box\n","        # draw.rectangle((first_index_x, first_index_y, last_index_x, last_index_y), outline = 'red')\n","\n","        # paste text on to the image\n","        img.paste(temp, temp)\n","\n","        # save image\n","        img.save(output)\n","        print(f'Image saved in {output}')\n","\n","        # calculate center_x, center_y, bbox_width, bbox_height\n","        # then normalize to (0-1)\n","        center_x = (last_index_x + first_index_x)/2\n","        center_x_norm = center_x / img.width\n","        center_y = (last_index_y + first_index_y)/2\n","        center_y_norm = center_y / img.height\n","        bbox_width = last_index_x - first_index_x\n","        bbox_width_norm = bbox_width / img.width\n","        bbox_height = last_index_y - first_index_y\n","        bbox_height_norm = bbox_height / img.height\n","\n","        return center_x_norm, center_y_norm, bbox_width_norm, bbox_height_norm"]},{"cell_type":"markdown","metadata":{"id":"a7ZyuaWgoDLE"},"source":["# Supplemental Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4MZ8NFOHOf8F"},"outputs":[],"source":["# create a GIF from the images\n","def create_gif(image_dir, gif_path, duration = 1):\n","    images = []\n","\n","    for filename in os.listdir(image_dir):\n","        file_path = os.path.join(image_dir, filename)\n","        images.append(imageio.v2.imread(file_path))\n","\n","    imageio.mimsave(gif_path, images, duration = duration)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2EyAvMvjOxMt"},"outputs":[],"source":["# create a mp4 video from the images\n","def create_video(image_dir, video_path, fps=1):\n","    images = []\n","\n","    for filename in os.listdir(image_dir):\n","        file_path = os.path.join(image_dir, filename)\n","        images.append(cv2.imread(file_path))\n","\n","    height, width, layers = images[0].shape\n","    video = cv2.VideoWriter(video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n","\n","    for image in images:\n","        video.write(image)\n","\n","    video.release()"]},{"cell_type":"markdown","metadata":{"id":"0B-zgtauoNcp"},"source":["# Generate Images for YOLO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X5ewOTCovKiw"},"outputs":[],"source":["# %%time\n","# # select images for modeling\n","# # copy to a new folder\n","# source_folder = '/content/drive/MyDrive/Gradient Health/Text Detection Project/Radiographs'\n","# destination_folder = '/content/drive/MyDrive/Gradient Health/Text Detection Project/Radiographs for Modeling'\n","\n","# # ensure the destination directory exists\n","# if not os.path.exists(destination_folder):\n","#     os.makedirs(destination_folder)\n","\n","# files = os.listdir(source_folder)\n","\n","# for file in files:\n","#     if file.endswith('000.png'):\n","#         source = os.path.join(source_folder, file)\n","#         destination = os.path.join(destination_folder, file)\n","\n","#         # Copy the file\n","#         shutil.copy(source, destination)"]},{"cell_type":"code","source":["# %%time\n","# # crop image to get rid of text-like feature noise\n","# source_folder = '/content/drive/MyDrive/Gradient Health/Text Detection Project/Radiographs for Modeling'\n","# files = os.listdir(source_folder)\n","\n","# for file in files:\n","#     source = os.path.join(source_folder, file)\n","#     try:\n","#         img = Image.open(source)\n","#         width, height = img.size # 1024 by 1024\n","#         left = 192\n","#         top = 192\n","#         right = 832\n","#         bottom = 832\n","#         img_cropped = img.crop((left, top, right, bottom)) # 640 by 640\n","#         img_cropped.save(source)\n","#     except Exception as e:\n","#         print(f\"Could not process file {file}: {str(e)}\")"],"metadata":{"id":"yGkvYKy_kRi9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check number of images available\n","input_path = '/content/drive/MyDrive/Gradient Health/Text Detection Project/Radiographs for Modeling'\n","file_count = sum([len(files) for r, d, files in os.walk(input_path)])\n","print(f\"Total number of files in the directory: {file_count}\")"],"metadata":{"id":"_nGruXXTrxh0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # rename images to '1.png', '2.png', ... , '1335.png'\n","# input_path = '/content/drive/MyDrive/Gradient Health/Text Detection Project/Radiographs for Modeling'\n","# files = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n","\n","# for idx, filename in enumerate(files, start=1):\n","#     old_file_path = os.path.join(input_path, filename)\n","#     new_file_path = os.path.join(input_path, f\"{idx}.png\")\n","#     os.rename(old_file_path, new_file_path)"],"metadata":{"id":"AzUe9rMq-D4q"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WS2Fftdv9iEF"},"outputs":[],"source":["# define main function parameters\n","languages = ['English', 'Portuguese', 'Chinese']\n","English_Portuguese_font_list = [\n","    'Arial Unicode.ttf',\n","    'Geneva.ttf',\n","    'Keyboard.ttf',\n","    'Monaco.ttf',\n","    'NewYork.ttf',\n","    'NewYorkItalic.ttf',\n","    'OpenSans-Light.ttf',\n","    'OpenSans_Condensed-Bold.ttf',\n","    'OpenSans_Condensed-BoldItalic.ttf',\n","    'OpenSans_Condensed-ExtraBold.ttf',\n","    'OpenSans_Condensed-ExtraBoldItalic.ttf',\n","    'OpenSans_Condensed-Italic.ttf',\n","    'OpenSans_Condensed-Light.ttf',\n","    'OpenSans_Condensed-LightItalic.ttf',\n","    'OpenSans_Condensed-Medium.ttf',\n","    'OpenSans_Condensed-MediumItalic.ttf',\n","    'OpenSans_Condensed-Regular.ttf',\n","    'OpenSans_Condensed-SemiBold.ttf',\n","    'OpenSans_Condensed-SemiBoldItalic.ttf',\n","    'OpenSans_SemiCondensed-Bold.ttf',\n","    'OpenSans_SemiCondensed-BoldItalic.ttf',\n","    'OpenSans_SemiCondensed-ExtraBold.ttf',\n","    'OpenSans_SemiCondensed-ExtraBoldItalic.ttf',\n","    'OpenSans_SemiCondensed-Italic.ttf',\n","    'OpenSans_SemiCondensed-Light.ttf',\n","    'OpenSans_SemiCondensed-LightItalic.ttf',\n","    'OpenSans_SemiCondensed-Medium.ttf',\n","    'OpenSans_SemiCondensed-MediumItalic.ttf',\n","    'OpenSans_SemiCondensed-Regular.ttf',\n","    'OpenSans_SemiCondensed-SemiBold.ttf',\n","    'OpenSans_SemiCondensed-SemiBoldItalic.ttf',\n","    'OpenSans-Bold.ttf',\n","    'OpenSans-BoldItalic.ttf',\n","    'OpenSans-ExtraBold.ttf',\n","    'OpenSans-ExtraBoldItalic.ttf',\n","    'OpenSans-Italic.ttf',\n","    'OpenSans-LightItalic.ttf',\n","    'OpenSans-Medium.ttf',\n","    'OpenSans-MediumItalic.ttf',\n","    'OpenSans-Regular.ttf',\n","    'OpenSans-SemiBold.ttf',\n","    'OpenSans-SemiBoldItalic.ttf',\n","    'Roboto-ThinItalic.ttf',\n","    'Roboto-Thin.ttf',\n","    'Roboto-Regular.ttf',\n","    'Roboto-MediumItalic.ttf',\n","    'Roboto-Medium.ttf',\n","    'Roboto-LightItalic.ttf',\n","    'Roboto-Light.ttf',\n","    'Roboto-Italic.ttf',\n","    'Roboto-BoldItalic.ttf',\n","    'Roboto-Bold.ttf',\n","    'Roboto-BlackItalic.ttf',\n","    'Roboto-Black.ttf',\n","    'SFCompact.ttf',\n","    'SFCompactItalic.ttf',\n","    'SFCompactRounded.ttf',\n","    'SFNS.ttf',\n","    'SFNSItalic.ttf',\n","    'SFNSMono.ttf',\n","    'SFNSMonoItalic.ttf',\n","    'SFNSRounded.ttf'\n","]\n","Chinese_font_list = [\n","    'NotoSansSC-Black.otf',\n","    'NotoSansSC-Bold.otf',\n","    'NotoSansSC-Light.otf',\n","    'NotoSansSC-Medium.otf',\n","    'NotoSansSC-Regular.otf',\n","    'NotoSansSC-Thin.otf',\n","    'HanyiSentyPagoda Regular.ttf',\n","    'HanyiSentyPine Regular.ttf',\n","    'HanyiSentyWen Regular.ttf'\n","]\n","font_size_range = (15,50)\n","stroke_width_range = (0,1)\n","rotation_degrees = [\n","    -2, -1, 0, 1, 2,\n","    88, 89, 90, 91, 92,\n","    268, 269, 270, 271, 272\n","]"]},{"cell_type":"markdown","metadata":{"id":"BAHE-8oB5faa"},"source":["## Generate Training Set (80%)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3x4iJVaohmL"},"outputs":[],"source":["%%time\n","# use index 1-1068 images\n","input_path = '/content/drive/MyDrive/Gradient Health/Text Detection Project/Radiographs for Modeling'\n","for i in range(1, 1069):\n","    # input\n","    image_name = str(i) + '.png'\n","    input = os.path.join(input_path, image_name)\n","    # output\n","    output = '/content/drive/MyDrive/Gradient Health/Text Detection Project/custom_dataset/images/train/train ' + str(i) + '.jpg'\n","    # language\n","    language = random.choice(languages)\n","    # font name\n","    if language == 'Chinese':\n","        font_name = random.choice(Chinese_font_list)\n","    else:\n","        font_name = random.choice(English_Portuguese_font_list)\n","    # font size\n","    font_size = random.randint(font_size_range[0], font_size_range[1])\n","    # text length\n","    text_length = generate_poisson()\n","    # fill color\n","    fill_color = generate_random_grayscale_color()\n","    # stroke color\n","    stroke_color = generate_random_grayscale_color()\n","    # stroke width\n","    stroke_width = random.randint(stroke_width_range[0], stroke_width_range[1])\n","    # rotation degree\n","    rotation_degree = random.choice(rotation_degrees)\n","\n","    # call main function\n","    center_x, center_y, bbox_width, bbox_height = overlay_random_text(\n","        input = input,\n","        output = output,\n","        language = language,\n","        font_name = font_name,\n","        font_size = font_size,\n","        text_length = text_length,\n","        fill_color = fill_color,\n","        stroke_color = stroke_color,\n","        stroke_width = stroke_width,\n","        rotation_degree = rotation_degree)\n","\n","    # save label to txt file with format (class, center_x, center_y, bbox_width, bbox_height)\n","    filename =  '/content/drive/MyDrive/Gradient Health/Text Detection Project/custom_dataset/labels/train/train ' + str(i) + '.txt'\n","    with open(filename, 'w') as file:\n","        file.write(f'0 {center_x} {center_y} {bbox_width} {bbox_height}')"]},{"cell_type":"markdown","metadata":{"id":"WUXycu9-5rjz"},"source":["## Generate Validation Set (20%)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DiLR1Gql6CXN"},"outputs":[],"source":["%%time\n","# use index 1069-1335 images\n","input_path = '/content/drive/MyDrive/Gradient Health/Text Detection Project/Radiographs for Modeling'\n","for i in range(1069, 1336):\n","    # input\n","    image_name = str(i) + '.png'\n","    input = os.path.join(input_path, image_name)\n","    # output\n","    output = '/content/drive/MyDrive/Gradient Health/Text Detection Project/custom_dataset/images/val/val ' + str(i-1068) + '.jpg'\n","    # language\n","    language = random.choice(languages)\n","    # font name\n","    if language == 'Chinese':\n","        font_name = random.choice(Chinese_font_list)\n","    else:\n","        font_name = random.choice(English_Portuguese_font_list)\n","    # font size\n","    font_size = random.randint(font_size_range[0], font_size_range[1])\n","    # text length\n","    text_length = generate_poisson()\n","    # fill color\n","    fill_color = generate_random_grayscale_color()\n","    # stroke color\n","    stroke_color = generate_random_grayscale_color()\n","    # stroke width\n","    stroke_width = random.randint(stroke_width_range[0], stroke_width_range[1])\n","    # rotation degree\n","    rotation_degree = random.choice(rotation_degrees)\n","\n","    # call main function\n","    center_x, center_y, bbox_width, bbox_height = overlay_random_text(\n","        input = input,\n","        output = output,\n","        language = language,\n","        font_name = font_name,\n","        font_size = font_size,\n","        text_length = text_length,\n","        fill_color = fill_color,\n","        stroke_color = stroke_color,\n","        stroke_width = stroke_width,\n","        rotation_degree = rotation_degree)\n","\n","    # save label to txt file with format (class, center_x, center_y, bbox_width, bbox_height)\n","    filename =  '/content/drive/MyDrive/Gradient Health/Text Detection Project/custom_dataset/labels/val/val ' + str(i-1068) + '.txt'\n","    with open(filename, 'w') as file:\n","        file.write(f'0 {center_x} {center_y} {bbox_width} {bbox_height}')"]},{"cell_type":"markdown","metadata":{"id":"vVsFRZ8c5rcN"},"source":["## Generate Testing Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YqpswttTXaTq"},"outputs":[],"source":["%%time\n","# generate 100 images\n","input_path = '/content/drive/MyDrive/Gradient Health/Text Detection Project/Radiographs'\n","input_files = os.listdir(input_path)\n","\n","for i in range(1,101):\n","    # input\n","    random_file = random.choice(input_files)\n","    input = os.path.join(input_path, random_file)\n","\n","    # Open the image file\n","    img = Image.open(input)\n","    width, height = img.size\n","\n","    # Ensure the crop box won't be out of the image boundaries\n","    left = random.randint(0, width - 640)\n","    upper = random.randint(0, height - 640)\n","    right = left + 640\n","    lower = upper + 640\n","\n","    # Crop the image\n","    cropped_img = img.crop((left, upper, right, lower))\n","\n","    # Convert the image to RGB\n","    rgb_img = cropped_img.convert('RGB')\n","\n","    # Save the cropped image temporarily\n","    temp_path = 'temp.jpg'\n","    rgb_img.save(temp_path)\n","\n","    # output\n","    output = '/content/drive/MyDrive/Gradient Health/Text Detection Project/custom_dataset/images/test/test ' + str(i) + '.jpg'\n","    # language\n","    language = random.choice(languages)\n","    # font name\n","    if language == 'Chinese':\n","        font_name = random.choice(Chinese_font_list)\n","    else:\n","        font_name = random.choice(English_Portuguese_font_list)\n","    # font size\n","    font_size = random.randint(font_size_range[0], font_size_range[1])\n","    # text length\n","    text_length = generate_poisson()\n","    # fill color\n","    fill_color = generate_random_grayscale_color()\n","    # stroke color\n","    stroke_color = generate_random_grayscale_color()\n","    # stroke width\n","    stroke_width = random.randint(stroke_width_range[0], stroke_width_range[1])\n","    # rotation degree\n","    rotation_degree = random.choice(rotation_degrees)\n","\n","    # call main function\n","    center_x, center_y, bbox_width, bbox_height = overlay_random_text(\n","        input = temp_path,\n","        output = output,\n","        language = language,\n","        font_name = font_name,\n","        font_size = font_size,\n","        text_length = text_length,\n","        fill_color = fill_color,\n","        stroke_color = stroke_color,\n","        stroke_width = stroke_width,\n","        rotation_degree = rotation_degree)\n","\n","    # save label to txt file with format (class, center_x, center_y, bbox_width, bbox_height)\n","    filename =  '/content/drive/MyDrive/Gradient Health/Text Detection Project/custom_dataset/labels/test/test ' + str(i) + '.txt'\n","    with open(filename, 'w') as file:\n","        file.write(f'0 {center_x} {center_y} {bbox_width} {bbox_height}')\n","\n","    # Remove the temporary image\n","    os.remove(temp_path)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"mount_file_id":"15R7BjWb11GDCPdyffYg5jR_5lXsw93VU","authorship_tag":"ABX9TyMD8b4XESfpxgcppmDBLrWp"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}